{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit       gpa         gre\n",
       "0      0  3.177277  594.102992\n",
       "1      0  3.412655  631.528607\n",
       "2      0  2.728097  553.714399\n",
       "3      0  3.093559  551.089985\n",
       "4      0  3.141923  537.184894"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "%matplotlib inline\n",
    "data = 'admission.csv'\n",
    "admissions = pd.read_csv(os.path.relpath(\"Data\\\\\"+data))\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<H3>Holdout Validation</H3>\n",
    "<p>The simplest way to evaluate a classifier's effectiveness is to:</p>\n",
    "<ul>\n",
    "<li>Randomly split data into training and test set</li>\n",
    "<li>fitting the model using the training set</li>\n",
    "<li>making predictions on the test set</li>\n",
    "</ul>\n",
    "<p>There's no hard and fast rule for how to split test and training data, for now, we'll use 80/20 for training and testing respectively. To do this we'll:</p>\n",
    "<ul>\n",
    "<li>use <a href=\"http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.random.permutation.html\">numpy.random.permutation</a> to return a list containing index values in random order</li>\n",
    "<li>return a new DataFrame in that list's order</li>\n",
    "<li>select the first 80% as the training set</li>\n",
    "<li>select the last 20% as the test set</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Seed RandomState instance\n",
    "np.random.seed(8)\n",
    "\n",
    "# Create list of randomized indices from DataFrame\n",
    "random_indices = list(np.random.permutation(admissions.index))\n",
    "\n",
    "# Create new DataFrame indexed in the random order\n",
    "shuffled_admissions = admissions.loc[random_indices]\n",
    "\n",
    "# Create training frame\n",
    "train = shuffled_admissions[:515]\n",
    "\n",
    "# Create test frame\n",
    "test = shuffled_admissions[515:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Accuracy</h3>\n",
    "<p>Now that the data is randomized, let's fit a logistic regression and evaluate it</p>\n",
    "\n",
    "<p>$Accuracy=\\frac{\\#\\:correctly\\:predicted}{\\#\\:of\\:Observations}$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6356589147286822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-haharv\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the regression model\n",
    "lr.fit(train[['gpa']],train['admit'])\n",
    "\n",
    "# Predict labels and add to test dataframe\n",
    "test['predicted_label'] = list(lr.predict(test[['gpa']]))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = test[test['predicted_label']==test['admit']].shape[0] / test.shape[0]\n",
    "\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Digging into accuracy</H3>\n",
    "<p>So the prediction accuracy in this model with the holdout set is ~64% which is close to the original estimate using the full set in the Logistic Regressions Basics notebook</p>\n",
    "<p>If the model performs <em>WAY WORSE</em> on the holdout data, it's a sign that we're overfitting. If the accuracy is much lower, then it may be a sign that the feature we're using isn't great or we should reconsider using logistic regression</p>\n",
    "\n",
    "<h4>Sensitivity and Specificity (again)</h4>\n",
    "<p>Sensitivity is the rate at which the model predicts true positives</p>\n",
    "<p>Specificity is the rate at which the model predicts true negatives</p>\n",
    "\n",
    "<p>$Sensitivity=\\frac{True\\:Positives}{True\\:Positives+False\\:Positives}$</p>\n",
    "\n",
    "<p>$Specificity=\\frac{True\\:Negatives}{True\\:Negatives+\\:False\\:Negatives}$</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n",
      "0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-haharv\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test['actual_label'] = test['admit']\n",
    "\n",
    "matches = test[\"predicted_label\"] == test[\"actual_label\"]\n",
    "correct_predictions = test[matches]\n",
    "\n",
    "true_positive_filter = (test[\"predicted_label\"] == 1) & (test[\"actual_label\"] == 1)\n",
    "true_positives = len(test[true_positive_filter])\n",
    "false_negative_filter = (test[\"predicted_label\"] == 0) & (test[\"actual_label\"] == 1)\n",
    "false_negatives = len(test[false_negative_filter])\n",
    "\n",
    "sensitivity = true_positives / (true_positives + false_negatives)\n",
    "print(sensitivity)\n",
    "\n",
    "false_positive_filter = (test[\"predicted_label\"] == 1) & (test[\"actual_label\"] == 0)\n",
    "false_positives = len(test[false_positive_filter])\n",
    "true_negative_filter = (test[\"predicted_label\"] == 0) & (test[\"actual_label\"] == 0)\n",
    "true_negatives = len(test[true_negative_filter])\n",
    "\n",
    "specificity = (true_negatives) / (false_positives + true_negatives)\n",
    "print(specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>False Positive Rate Deeper Dive</H3>\n",
    "<p>When we use the LogisticRegression.predict method, scikit-learn uses the logit function, $\\sigma(t)=\\frac{e^t}{1+e^t}$, to determine the probability value of each observation, then assigns a 1/0 if it's greater than a threshold (50% by default). For most problems, though, 50% is not the optimal $discrimination\\:threshold$ so accordingly, we would ideally have a way to compute measures at varied threshold values and optimize our model depending on our objectives.</p>\n",
    "\n",
    "<p>As stated above, we have two fundamental measures for a given discrimination threshold:</p>\n",
    "<ol>\n",
    "<li>False Positive Rate (also known as Fall-out rate)</li>\n",
    "<li>True Positive Rate</li>\n",
    "</ol>\n",
    "\n",
    "<H3>The ROC Curve</H3>\n",
    "<p>We can vary the discrimination threshold and calculate the TPR and FPR for each threshold. This results in an $ROC\\:curve$ which stands for $Receiver\\:Operator\\:Curve$, which allows us to determine the classification model's performace for all values of the discrimination threshold</p>\n",
    "<p>For this we use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\">scikit-learn ````roc_curve```` function</a></p>\n",
    "<p>The roc_curve function takes two requred parameters ````y_true: list of true labels```` and ````y_score: list of probability scores for those observations````</p>\n",
    "<p>The ````roc_curve```` function returns 3 values that you can assign to separate variables when you call the function ````fpr, tpr, thresholds = metrics.roc_curve(labels, probabilities)````</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAERCAYAAABhKjCtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb1d2990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import roc_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Generate probs for model from observations\n",
    "probs = lr.predict_proba(test[['gpa']])\n",
    "\n",
    "# use roc_curve to generate fpr, tpr, and thresholds variable\n",
    "fpr,tpr,thresholds = roc_curve(list(test['actual_label']),probs[:,1])\n",
    "\n",
    "# Plot ROC\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3>Area Under the Curve</H3>\n",
    "<p>Looking at the ROC curve above, you can see the tradeoff of increasing the true positive rate--as that increases, so does the false positive rate, since they both converge to 1</p>\n",
    "<p>Going further, we can calculate the $Area\\:under\\: the\\: curve$ which describes the probability that the classifier will catch a random positive observation higher than a random negative observation</p>\n",
    "<p>We can use the scikit-learn function <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\">````roc_auc_score````</a> which takes the same paramaters as the ````roc_curve```` function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.577932098765\n",
      "our model is slightly better than flipping a coin!!!!\n"
     ]
    }
   ],
   "source": [
    "# Import auc_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(list(test['actual_label']),probs[:,1])\n",
    "\n",
    "print(auc_score)\n",
    "print(\"our model is slightly better than flipping a coin!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
